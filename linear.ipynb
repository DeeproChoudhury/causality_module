{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'C'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3891607/3227602149.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Initialize and fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Predictions on the training set (or a test set if you split your data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1385\u001b[0m                 skip_parameter_validation=(\n\u001b[1;32m   1386\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m             \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0maccept_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositive\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coo\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m         X, y = validate_data(\n\u001b[0m\u001b[1;32m    602\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2957\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"estimator\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2958\u001b[0m                 \u001b[0mcheck_y_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2961\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2962\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2964\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0mensure_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deprecate_force_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1371\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1052\u001b[0m                         \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m                 raise ValueError(\n\u001b[1;32m   1058\u001b[0m                     \u001b[0;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m                 \u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m   2149\u001b[0m     def __array__(\n\u001b[1;32m   2150\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m     \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2153\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2154\u001b[0m         if (\n\u001b[1;32m   2155\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'C'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"datAY.csv\")\n",
    "\n",
    "predictors = ['A'] + [f'x_{i}' for i in range(1, 59)]\n",
    "X = data[predictors]\n",
    "y = data['Y']\n",
    "\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(\"Training MSE:\", mse)\n",
    "\n",
    "\n",
    "X1 = X.copy()\n",
    "X1['A'] = 1\n",
    "X0 = X.copy()\n",
    "X0['A'] = 0\n",
    "\n",
    "Q1 = model.predict(X1)\n",
    "Q0 = model.predict(X0)\n",
    "\n",
    "\n",
    "estimated_ate = (Q1 - Q0).mean()\n",
    "print(\"Estimated ATE:\", estimated_ate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 83333.01351087335\n",
      "Estimated ATE: -521.9398861268523\n",
      "Estimated ATE Standard Deviation: 2.269708454168276e-13\n",
      "average ate - 3 * std:  -521.939886126853\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"datAY.csv\", index_col=0)\n",
    "\n",
    "\n",
    "predictors = ['A', 'x_1', 'x_3', 'x_4']\n",
    "X = data[predictors]\n",
    "y = data['Y']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(\"Training MSE:\", mse)\n",
    "\n",
    "\n",
    "X_treated = X.copy()\n",
    "X_control = X.copy()\n",
    "X_treated['A'] = 1   # All observations as treated\n",
    "X_control['A'] = 0   # All observations as control\n",
    "\n",
    "Q1 = model.predict(X_treated)\n",
    "Q0 = model.predict(X_control)\n",
    "\n",
    "\n",
    "estimated_ate = (Q1 - Q0).mean()\n",
    "estimated_ate_std = (Q1 - Q0).std()\n",
    "print(\"Estimated ATE:\", estimated_ate)\n",
    "print(\"Estimated ATE Standard Deviation:\", estimated_ate_std)\n",
    "print(\"average ate - 3 * std: \", estimated_ate - 3 * estimated_ate_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propensity scores: [0.43158542 0.34286201 0.34286201 0.40859634 0.56254877 0.68255377\n",
      " 0.51269955 0.38175895 0.41328815 0.36859523]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"datAY.csv\", index_col=0)\n",
    "\n",
    "\n",
    "predictors = ['x_1', 'x_3', 'x_4']\n",
    "X_ps = data[predictors]\n",
    "A = data['A']\n",
    "\n",
    "\n",
    "propensity_model = LogisticRegression(solver='liblinear')\n",
    "propensity_model.fit(X_ps, A)\n",
    "\n",
    "\n",
    "propensity_scores = propensity_model.predict_proba(X_ps)[:, 1]\n",
    "\n",
    "\n",
    "print(\"Propensity scores:\", propensity_scores[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPTW estimator: -538.6720042701767\n",
      "IPTW estimator standard deviation: 6111.278774651677\n",
      "IPTW estimator - 3 * std:  -18872.50832822521\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"datAY.csv\", index_col=0)\n",
    "\n",
    "\n",
    "predictors = ['x_1', 'x_3', 'x_4']\n",
    "X_ps = data[predictors]\n",
    "A = data['A']\n",
    "Y = data['Y']\n",
    "\n",
    "\n",
    "propensity_model = LogisticRegression(solver='liblinear')\n",
    "propensity_model.fit(X_ps, A)\n",
    "\n",
    "propensity_scores = propensity_model.predict_proba(X_ps)[:, 1]\n",
    "\n",
    "\n",
    "eps = 1e-6\n",
    "propensity_scores = np.clip(propensity_scores, eps, 1 - eps)\n",
    "\n",
    "IPTW = np.mean((Y * A / propensity_scores) - (Y * (1 - A) / (1 - propensity_scores)))\n",
    "IPTW_std = np.std((Y * A / propensity_scores) - (Y * (1 - A) / (1 - propensity_scores)))\n",
    "print(\"IPTW estimator:\", IPTW)\n",
    "print(\"IPTW estimator standard deviation:\", IPTW_std)\n",
    "print(\"IPTW estimator - 3 * std: \", IPTW - 3 * IPTW_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stabilized h-IPTW estimator: -520.3033329316172\n",
      "Stabilized h-IPTW estimator standard deviation: 50.61841346786772\n",
      "Stabilized h-IPTW estimator - 3 * std:  -672.1585733352204\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"datAY.csv\", index_col=0)\n",
    "\n",
    "\n",
    "predictors = ['x_1', 'x_3', 'x_4']\n",
    "X_ps = data[predictors]\n",
    "A = data['A']\n",
    "Y = data['Y']\n",
    "\n",
    "\n",
    "propensity_model = LogisticRegression(solver='liblinear')\n",
    "propensity_model.fit(X_ps, A)\n",
    "propensity_scores = propensity_model.predict_proba(X_ps)[:, 1]\n",
    "\n",
    "\n",
    "eps = 1e-6\n",
    "propensity_scores = np.clip(propensity_scores, eps, 1 - eps)\n",
    "\n",
    "\n",
    "\n",
    "treated_numerator = np.sum(Y * A / propensity_scores)\n",
    "treated_denominator = np.sum(A / propensity_scores)\n",
    "treated_mean = treated_numerator / treated_denominator\n",
    "\n",
    "\n",
    "control_numerator = np.sum(Y * (1 - A) / (1 - propensity_scores))\n",
    "control_denominator = np.sum((1 - A) / (1 - propensity_scores))\n",
    "control_mean = control_numerator / control_denominator\n",
    "\n",
    "\n",
    "tau_h_iptw = treated_mean - control_mean\n",
    "\n",
    "print(\"Stabilized h-IPTW estimator:\", tau_h_iptw)\n",
    "std = np.sqrt(np.var(Y * A / propensity_scores) / np.sum(A / propensity_scores) + np.var(Y * (1 - A) / (1 - propensity_scores) / np.sum((1 - A) / (1 - propensity_scores))))\n",
    "print(\"Stabilized h-IPTW estimator standard deviation:\", std)\n",
    "print(\"Stabilized h-IPTW estimator - 3 * std: \", tau_h_iptw - 3 * std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h-IPTW estimator: -520.3033329316172\n",
      "Standard deviation of the h-IPTW estimator: 8.80431767382781\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "data = pd.read_csv(\"datAY.csv\", index_col=0)\n",
    "\n",
    "def compute_h_iptw(df):\n",
    "    predictors = ['x_1', 'x_3', 'x_4']\n",
    "    X_ps = df[predictors]\n",
    "    A = df['A']\n",
    "    Y = df['Y']\n",
    "    \n",
    "    propensity_model = LogisticRegression(solver='liblinear')\n",
    "    propensity_model.fit(X_ps, A)\n",
    "    propensity_scores = propensity_model.predict_proba(X_ps)[:, 1]\n",
    "    \n",
    "    eps = 1e-6\n",
    "    propensity_scores = np.clip(propensity_scores, eps, 1 - eps)\n",
    "    \n",
    "    treated_numerator = np.sum(Y * A / propensity_scores)\n",
    "    treated_denominator = np.sum(A / propensity_scores)\n",
    "    treated_mean = treated_numerator / treated_denominator\n",
    "\n",
    "    control_numerator = np.sum(Y * (1 - A) / (1 - propensity_scores))\n",
    "    control_denominator = np.sum((1 - A) / (1 - propensity_scores))\n",
    "    control_mean = control_numerator / control_denominator\n",
    "    \n",
    "    return treated_mean - control_mean\n",
    "\n",
    "tau_h_iptw = compute_h_iptw(data)\n",
    "print(\"h-IPTW estimator:\", tau_h_iptw)\n",
    "\n",
    "B = 1000\n",
    "bootstrap_estimates = []\n",
    "n = data.shape[0]\n",
    "\n",
    "for b in range(B):\n",
    "    bootstrap_sample = data.sample(n, replace=True)\n",
    "    estimate = compute_h_iptw(bootstrap_sample)\n",
    "    bootstrap_estimates.append(estimate)\n",
    "\n",
    "\n",
    "std_est = np.std(bootstrap_estimates, ddof=1)\n",
    "print(\"Standard deviation of the h-IPTW estimator:\", std_est)\n",
    "print(\"h-IPTW estimator - 3 * std: \", tau_h_iptw - 3 * std_est)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h-IPTW estimator: -520.3033329316172\n",
      "Sandwich variance-based SE for h-IPTW: 0.0025553243214472003\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"datAY.csv\", index_col=0)\n",
    "\n",
    "predictors = ['x_1', 'x_3', 'x_4']\n",
    "X_ps = data[predictors]\n",
    "A = data['A'].values\n",
    "Y = data['Y'].values\n",
    "\n",
    "\n",
    "propensity_model = LogisticRegression(solver='liblinear')\n",
    "propensity_model.fit(X_ps, A)\n",
    "g_hat = propensity_model.predict_proba(X_ps)[:, 1]\n",
    "\n",
    "\n",
    "eps = 1e-6\n",
    "g_hat = np.clip(g_hat, eps, 1 - eps)\n",
    "\n",
    "\n",
    "W1 = A / g_hat\n",
    "W0 = (1 - A) / (1 - g_hat)\n",
    "\n",
    "S1 = np.sum(W1)\n",
    "S0 = np.sum(W0)\n",
    "\n",
    "\n",
    "mu1 = np.sum(W1 * Y) / S1\n",
    "mu0 = np.sum(W0 * Y) / S0\n",
    "\n",
    "tau_hat = mu1 - mu0\n",
    "print(\"h-IPTW estimator:\", tau_hat)\n",
    "\n",
    "# Approximate influence functions:\n",
    "IF1 = (W1 / S1) * (Y - mu1)\n",
    "IF0 = (W0 / S0) * (Y - mu0)\n",
    "IF_tau = IF1 - IF0  # Influence function for tau_hat\n",
    "\n",
    "# Sandwich variance estimator:\n",
    "n = len(Y)\n",
    "var_tau = np.var(IF_tau, ddof=1) / n\n",
    "se_tau = np.sqrt(var_tau)\n",
    "print(\"Sandwich variance-based SE for h-IPTW:\", se_tau)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doubleml as dml\n",
    "\n",
    "datAY = pd.read_csv(\"datAY.csv\")\n",
    "data = datAY[['x_1', 'x_3', 'x_4', 'A', 'Y']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE estimate: [-585.39663319]\n",
      "ATE robust (sandwich) standard error: [14.73444534]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from doubleml import DoubleMLData, DoubleMLPLR, DoubleMLIRM\n",
    "from sklearn.linear_model import LassoCV, LogisticRegressionCV\n",
    "from abess.linear import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "data = pd.read_csv(\"new_datAY.csv\", index_col=0)\n",
    "\n",
    "x_cols = ['x_1', 'x_3', 'x_4']\n",
    "\n",
    "dml_data = DoubleMLData(data, y_col='Y', d_cols='A', x_cols=x_cols)\n",
    "\n",
    "# ml_g = LogisticRegressionCV(cv=5)\n",
    "ml_g = LinearRegression(cv=5)\n",
    "ml_m = LassoCV(cv=5)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "dml_plr = DoubleMLPLR(dml_data, ml_g, ml_m)\n",
    "\n",
    "dml_plr.fit()\n",
    "\n",
    "ate_estimate = dml_plr.coef\n",
    "ate_se = dml_plr.se\n",
    "\n",
    "print(\"ATE estimate:\", ate_estimate)\n",
    "print(\"ATE robust (sandwich) standard error:\", ate_se)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         coef   std err          t  P>|t|       2.5 %      97.5 %\n",
      "A -521.405467  8.767086 -59.473061    0.0 -538.588641 -504.222294\n"
     ]
    }
   ],
   "source": [
    "# df = dgp()\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "aipw_obj_1 = DoubleMLIRM(dml_data, ml_g = LinearRegression(cv=5),\n",
    "                      ml_m = LogisticRegressionCV(cv=5), n_folds=5)\n",
    "aipw_obj_1.fit()\n",
    "\n",
    "aipw_est_1 = aipw_obj_1.summary\n",
    "print(aipw_est_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/choudhury/miniconda3/envs/post_env/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m gp_c \u001b[38;5;241m=\u001b[39m GaussianProcessClassifier(kernel\u001b[38;5;241m=\u001b[39mkernel, n_restarts_optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m obj_1 \u001b[38;5;241m=\u001b[39m DoubleMLIRM(dml_data, ml_g \u001b[38;5;241m=\u001b[39m gp, ml_m \u001b[38;5;241m=\u001b[39m gp_c, n_folds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mobj_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m est_1 \u001b[38;5;241m=\u001b[39m obj_1\u001b[38;5;241m.\u001b[39msummary\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(est_1)\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/doubleml/double_ml.py:503\u001b[0m, in \u001b[0;36mDoubleML.fit\u001b[0;34m(self, n_jobs_cv, store_predictions, external_predictions, store_models)\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dml_data\u001b[38;5;241m.\u001b[39mset_x_d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dml_data\u001b[38;5;241m.\u001b[39md_cols[i_d])\n\u001b[1;32m    502\u001b[0m \u001b[38;5;66;03m# predictions have to be stored in loop for sensitivity analysis\u001b[39;00m\n\u001b[0;32m--> 503\u001b[0m nuisance_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_nuisance_and_score_elements\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs_cv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstore_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexternal_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstore_models\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_solve_score_and_estimate_se()\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# sensitivity elements can depend on the estimated parameter\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/doubleml/double_ml.py:966\u001b[0m, in \u001b[0;36mDoubleML._fit_nuisance_and_score_elements\u001b[0;34m(self, n_jobs_cv, store_predictions, external_predictions, store_models)\u001b[0m\n\u001b[1;32m    960\u001b[0m ext_prediction_dict \u001b[38;5;241m=\u001b[39m _set_external_predictions(external_predictions,\n\u001b[1;32m    961\u001b[0m                                                 learners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams_names,\n\u001b[1;32m    962\u001b[0m                                                 treatment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dml_data\u001b[38;5;241m.\u001b[39md_cols[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_i_treat],\n\u001b[1;32m    963\u001b[0m                                                 i_rep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_i_rep)\n\u001b[1;32m    965\u001b[0m \u001b[38;5;66;03m# ml estimation of nuisance models and computation of score elements\u001b[39;00m\n\u001b[0;32m--> 966\u001b[0m score_elements, preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nuisance_est\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__smpls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs_cv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mexternal_predictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mext_prediction_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mreturn_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore_models\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_score_elements(score_elements, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_i_rep, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_i_treat)\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# calculate nuisance losses and store predictions and targets of the nuisance models\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/doubleml/irm/irm.py:303\u001b[0m, in \u001b[0;36mDoubleMLIRM._nuisance_est\u001b[0;34m(self, smpls, n_jobs_cv, external_predictions, return_models)\u001b[0m\n\u001b[1;32m    299\u001b[0m     m_hat \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m'\u001b[39m: external_predictions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml_m\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    300\u001b[0m              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    301\u001b[0m              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     m_hat \u001b[38;5;241m=\u001b[39m \u001b[43m_dml_cv_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_learner\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mml_m\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmpls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msmpls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs_cv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mest_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_params\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mml_m\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_method\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mml_m\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mreturn_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_models\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     _check_finite_predictions(m_hat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learner[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml_m\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml_m\u001b[39m\u001b[38;5;124m'\u001b[39m, smpls)\n\u001b[1;32m    307\u001b[0m     _check_is_propensity(m_hat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learner[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml_m\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml_m\u001b[39m\u001b[38;5;124m'\u001b[39m, smpls, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-12\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/doubleml/utils/_estimation.py:63\u001b[0m, in \u001b[0;36m_dml_cv_predict\u001b[0;34m(estimator, x, y, smpls, n_jobs, est_params, method, return_train_preds, return_models)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m manual_cv_predict:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;66;03m# if there are no parameters set we redirect to the standard method\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m         preds \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msmpls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(est_params, \u001b[38;5;28mdict\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:1282\u001b[0m, in \u001b[0;36mcross_val_predict\u001b[0;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, params, pre_dispatch, method)\u001b[0m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m   1281\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m-> 1282\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_predict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m inv_test_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(test_indices), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m   1296\u001b[0m inv_test_indices[test_indices] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(test_indices))\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:1367\u001b[0m, in \u001b[0;36m_fit_and_predict\u001b[0;34m(estimator, X, y, train, test, fit_params, method)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1367\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1368\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(estimator, method)\n\u001b[1;32m   1369\u001b[0m predictions \u001b[38;5;241m=\u001b[39m func(X_test)\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/sklearn/gaussian_process/_gpc.py:741\u001b[0m, in \u001b[0;36mGaussianProcessClassifier.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    739\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown multi-class mode \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_class)\n\u001b[0;32m--> 741\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_estimator_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_marginal_likelihood_value_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\n\u001b[1;32m    745\u001b[0m         [\n\u001b[1;32m    746\u001b[0m             estimator\u001b[38;5;241m.\u001b[39mlog_marginal_likelihood()\n\u001b[1;32m    747\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m estimator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_estimator_\u001b[38;5;241m.\u001b[39mestimators_\n\u001b[1;32m    748\u001b[0m         ]\n\u001b[1;32m    749\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/sklearn/gaussian_process/_gpc.py:229\u001b[0m, in \u001b[0;36m_BinaryGaussianProcessClassifierLaplace.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_marginal_likelihood(theta, clone_kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# First optimize starting from theta specified in kernel\u001b[39;00m\n\u001b[1;32m    228\u001b[0m optima \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constrained_optimization\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbounds\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m ]\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Additional runs are performed from log-uniform chosen initial\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# theta\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_restarts_optimizer \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/sklearn/gaussian_process/_gpc.py:474\u001b[0m, in \u001b[0;36m_BinaryGaussianProcessClassifierLaplace._constrained_optimization\u001b[0;34m(self, obj_func, initial_theta, bounds)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_constrained_optimization\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj_func, initial_theta, bounds):\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmin_l_bfgs_b\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 474\u001b[0m         opt_res \u001b[38;5;241m=\u001b[39m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_theta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m         _check_optimize_result(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m\"\u001b[39m, opt_res)\n\u001b[1;32m    478\u001b[0m         theta_opt, func_min \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/scipy/optimize/_minimize.py:738\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    735\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    736\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 738\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    741\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    742\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/scipy/optimize/_lbfgsb_py.py:441\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    433\u001b[0m _lbfgsb\u001b[38;5;241m.\u001b[39msetulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr, pgtol, wa,\n\u001b[1;32m    434\u001b[0m                iwa, task, lsave, isave, dsave, maxls, ln_task)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 441\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[1;32m    444\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:344\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x(x)\n\u001b[0;32m--> 344\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:295\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 295\u001b[0m         fx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m fx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lowest_f:\n\u001b[1;32m    297\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lowest_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:21\u001b[0m, in \u001b[0;36m_wrapper_fun.<locals>.wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     17\u001b[0m ncalls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/scipy/optimize/_optimize.py:80\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/scipy/optimize/_optimize.py:74\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 74\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/sklearn/gaussian_process/_gpc.py:220\u001b[0m, in \u001b[0;36m_BinaryGaussianProcessClassifierLaplace.fit.<locals>.obj_func\u001b[0;34m(theta, eval_gradient)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mobj_func\u001b[39m(theta, eval_gradient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_gradient:\n\u001b[0;32m--> 220\u001b[0m         lml, grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_marginal_likelihood\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_gradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclone_kernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mlml, \u001b[38;5;241m-\u001b[39mgrad\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/sklearn/gaussian_process/_gpc.py:385\u001b[0m, in \u001b[0;36m_BinaryGaussianProcessClassifierLaplace.log_marginal_likelihood\u001b[0;34m(self, theta, eval_gradient, clone_kernel)\u001b[0m\n\u001b[1;32m    381\u001b[0m     K \u001b[38;5;241m=\u001b[39m kernel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train_)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Compute log-marginal-likelihood Z and also store some temporaries\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# which can be reused for computing Z's gradient\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m Z, (pi, W_sr, L, b, a) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_posterior_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_temporaries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m eval_gradient:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Z\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/sklearn/gaussian_process/_gpc.py:444\u001b[0m, in \u001b[0;36m_BinaryGaussianProcessClassifierLaplace._posterior_mode\u001b[0;34m(self, K, return_temporaries)\u001b[0m\n\u001b[1;32m    442\u001b[0m W_sr_K \u001b[38;5;241m=\u001b[39m W_sr[:, np\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;241m*\u001b[39m K\n\u001b[1;32m    443\u001b[0m B \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meye(W\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m W_sr_K \u001b[38;5;241m*\u001b[39m W_sr\n\u001b[0;32m--> 444\u001b[0m L \u001b[38;5;241m=\u001b[39m \u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# Line 6\u001b[39;00m\n\u001b[1;32m    446\u001b[0m b \u001b[38;5;241m=\u001b[39m W \u001b[38;5;241m*\u001b[39m f \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train_ \u001b[38;5;241m-\u001b[39m pi)\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/scipy/linalg/_decomp_cholesky.py:101\u001b[0m, in \u001b[0;36mcholesky\u001b[0;34m(a, lower, overwrite_a, check_finite)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcholesky\u001b[39m(a, lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, overwrite_a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, check_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m    Compute the Cholesky decomposition of a matrix.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m \n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     c, lower \u001b[38;5;241m=\u001b[39m \u001b[43m_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_finite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/scipy/linalg/_decomp_cholesky.py:36\u001b[0m, in \u001b[0;36m_cholesky\u001b[0;34m(a, lower, overwrite_a, clean, check_finite)\u001b[0m\n\u001b[1;32m     34\u001b[0m overwrite_a \u001b[38;5;241m=\u001b[39m overwrite_a \u001b[38;5;129;01mor\u001b[39;00m _datacopied(a1, a)\n\u001b[1;32m     35\u001b[0m potrf, \u001b[38;5;241m=\u001b[39m get_lapack_funcs((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpotrf\u001b[39m\u001b[38;5;124m'\u001b[39m,), (a1,))\n\u001b[0;32m---> 36\u001b[0m c, info \u001b[38;5;241m=\u001b[39m \u001b[43mpotrf\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-th leading minor of the array is not positive \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefinite\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m info)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor, GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "\n",
    "kernel = 1.0 * RBF(length_scale=1.0) + WhiteKernel(noise_level=1.0)\n",
    "\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=1, normalize_y=True)\n",
    "gp_c = GaussianProcessClassifier(kernel=kernel, n_restarts_optimizer=1)\n",
    "\n",
    "obj_1 = DoubleMLIRM(dml_data, ml_g = gp, ml_m = gp_c, n_folds=5)\n",
    "\n",
    "obj_1.fit()\n",
    "\n",
    "est_1 = obj_1.summary\n",
    "print(est_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE estimate: [-586.76826293]\n",
      "ATE robust (sandwich) standard error: [16.68379162]\n",
      "         coef    std err          t          P>|t|       2.5 %      97.5 %\n",
      "A -586.768263  16.683792 -35.169959  5.759041e-271 -619.467894 -554.068632\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from doubleml import DoubleMLData, DoubleMLPLR\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "\n",
    "\n",
    "ml_m = MLPRegressor(hidden_layer_sizes=(32, 32), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
    "\n",
    "\n",
    "ml_g = MLPClassifier(hidden_layer_sizes=(32, 32), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "dml_plr = DoubleMLPLR(dml_data, ml_m, ml_g, n_folds=10)\n",
    "\n",
    "\n",
    "dml_plr.fit()\n",
    "\n",
    "\n",
    "ate_estimate = dml_plr.coef\n",
    "ate_se = dml_plr.se\n",
    "\n",
    "print(\"ATE estimate:\", ate_estimate)\n",
    "print(\"ATE robust (sandwich) standard error:\", ate_se)\n",
    "\n",
    "est2 = dml_plr.summary\n",
    "print(est2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1, 3, 4, 6, 9, 10, 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE estimate: [-440.89621486]\n",
      "ATE robust (sandwich) standard error: [12.568061]\n",
      "         coef    std err          t          P>|t|       2.5 %      97.5 %\n",
      "A -440.896215  12.568061 -35.080687  1.328214e-269 -465.529162 -416.263268\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv(\"new_datAY_2.csv\", index_col=0)\n",
    "\n",
    "x_cols = ['x_1', 'x_3', 'x_4', 'x_5', 'x_6', 'x_7', 'x_8', 'x_9', 'x_10', 'x_12', 'x_13', 'x_15', 'x_16', 'x_17', 'x_18', 'x_19', 'x_20', 'x_22', \n",
    "          'x_23', 'x_25', 'x_26', 'x_27', 'x_28', 'x_29', 'x_30', 'x_31', 'x_32', 'x_33', 'x_34', 'x_35', 'x_36', 'x_37', 'x_38', 'x_39', 'x_40', 'x_41', 'x_42', 'x_43', \n",
    "          'x_44', 'x_45', 'x_46', 'x_47', 'x_48', 'x_49', 'x_50', 'x_51', 'x_52', 'x_53', 'x_54', 'x_55', 'x_56', 'x_57', 'x_58']\n",
    "\n",
    "# x_cols = ['x_1', 'x_3', 'x_4', 'x_6', 'x_9', 'x_10', 'x_12']\n",
    "\n",
    "dml_data = DoubleMLData(data, y_col='Y', d_cols='A', x_cols=x_cols)\n",
    "\n",
    "ml_m = MLPRegressor(hidden_layer_sizes=(64, 64), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
    "\n",
    "\n",
    "ml_g = MLPClassifier(hidden_layer_sizes=(64, 64), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "dml_plr = DoubleMLPLR(dml_data, ml_m, ml_g, n_folds=10)\n",
    "\n",
    "\n",
    "dml_plr.fit()\n",
    "\n",
    "\n",
    "ate_estimate = dml_plr.coef\n",
    "ate_se = dml_plr.se\n",
    "\n",
    "print(\"ATE estimate:\", ate_estimate)\n",
    "print(\"ATE robust (sandwich) standard error:\", ate_se)\n",
    "\n",
    "est2 = dml_plr.summary\n",
    "print(est2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Apply sensitivity_analysis() to include senario in sensitivity_plot. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[204], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# dml_plr.sensitvity_analysis()\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdml_plr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msensitivity_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/doubleml/double_ml.py:1501\u001b[0m, in \u001b[0;36mDoubleML.sensitivity_plot\u001b[0;34m(self, idx_treatment, value, rho, level, null_hypothesis, include_scenario, benchmarks, fill, grid_bounds, grid_size)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mApply fit() before sensitivity_plot().\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1501\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_framework\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msensitivity_plot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43midx_treatment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx_treatment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnull_hypothesis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnull_hypothesis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_scenario\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_scenario\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbenchmarks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbenchmarks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid_size\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fig\n",
      "File \u001b[0;32m~/miniconda3/envs/post_env/lib/python3.12/site-packages/doubleml/double_ml_framework.py:870\u001b[0m, in \u001b[0;36mDoubleMLFramework.sensitivity_plot\u001b[0;34m(self, idx_treatment, value, rho, level, null_hypothesis, include_scenario, benchmarks, fill, grid_bounds, grid_size)\u001b[0m\n\u001b[1;32m    868\u001b[0m _check_bool(include_scenario, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minclude_scenario\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_scenario \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msensitivity_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 870\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mApply sensitivity_analysis() to include senario in sensitivity_plot. \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    871\u001b[0m _check_benchmarks(benchmarks)\n\u001b[1;32m    872\u001b[0m _check_bool(fill, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfill\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Apply sensitivity_analysis() to include senario in sensitivity_plot. "
     ]
    }
   ],
   "source": [
    "# dml_plr.sensitvity_analysis()\n",
    "dml_plr.sensitivity_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE estimate: [-1004.61863032]\n",
      "ATE robust (sandwich) standard error: [45.00125123]\n"
     ]
    }
   ],
   "source": [
    "# data = pd.read_csv(\"datAY.csv\", index_col=0)\n",
    "data = pd.read_csv(\"more_covariates_balanced.csv\", index_col=0)\n",
    "\n",
    "x_cols = ['x_1', 'x_3', 'x_4', 'x_6', 'x_9', 'x_10', 'x_12']\n",
    "\n",
    "dml_data = DoubleMLData(data, y_col='Y', d_cols='A', x_cols=x_cols)\n",
    "\n",
    "# ml_g = LogisticRegressionCV(cv=5)\n",
    "ml_g = LinearRegression(cv=5)\n",
    "ml_m = LassoCV(cv=5)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "dml_plr = DoubleMLPLR(dml_data, ml_g, ml_m)\n",
    "\n",
    "dml_plr.fit()\n",
    "\n",
    "\n",
    "ate_estimate = dml_plr.coef\n",
    "ate_se = dml_plr.se\n",
    "\n",
    "print(\"ATE estimate:\", ate_estimate)\n",
    "print(\"ATE robust (sandwich) standard error:\", ate_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"datAY.csv\", index_col=0)\n",
    "\n",
    "\n",
    "predictors = ['x_1', 'x_3', 'x_4', 'x_6', 'x_9', 'x_10']\n",
    "X_ps = data[predictors]\n",
    "A = data['A'].values\n",
    "Y = data['Y'].values\n",
    "\n",
    "propensity_model = LogisticRegression(solver='liblinear')\n",
    "propensity_model.fit(X_ps, A)\n",
    "g_hat = propensity_model.predict_proba(X_ps)[:, 1]\n",
    "\n",
    "\n",
    "eps = 1e-6\n",
    "g_hat = np.clip(g_hat, eps, 1 - eps)\n",
    "\n",
    "\n",
    "W1 = A / g_hat\n",
    "W0 = (1 - A) / (1 - g_hat)\n",
    "\n",
    "S1 = np.sum(W1)\n",
    "S0 = np.sum(W0)\n",
    "\n",
    "mu1 = np.sum(W1 * Y) / S1\n",
    "mu0 = np.sum(W0 * Y) / S0\n",
    "\n",
    "tau_hat = mu1 - mu0\n",
    "print(\"h-IPTW estimator:\", tau_hat)\n",
    "\n",
    "IF1 = (W1 / S1) * (Y - mu1)\n",
    "IF0 = (W0 / S0) * (Y - mu0)\n",
    "IF_tau = IF1 - IF0 \n",
    "\n",
    "n = len(Y)\n",
    "var_tau = np.var(IF_tau, ddof=1) / n\n",
    "se_tau = np.sqrt(var_tau)\n",
    "print(\"Sandwich variance-based SE for h-IPTW:\", se_tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x_1', 'x_2', 'x_3', 'x_4', 'x_5', 'x_6', 'x_7', 'x_8', 'x_9', 'x_10', 'x_12', 'x_13', 'x_15', 'x_16', 'x_17', 'x_18', 'x_19', 'x_20', 'x_22', 'x_23', 'x_25', 'x_26', 'x_27', 'x_28', 'x_29', 'x_30', 'x_31', 'x_32', 'x_33', 'x_34', 'x_35', 'x_36', 'x_37', 'x_38', 'x_39', 'x_40', 'x_41', 'x_42', 'x_43', 'x_44', 'x_45', 'x_46', 'x_47', 'x_48', 'x_49', 'x_50', 'x_51', 'x_52', 'x_53', 'x_54', 'x_55', 'x_56', 'x_57', 'x_58', 'A', 'Y', 'prop_score', 'x_11_1', 'x_11_2', 'x_11_3', 'x_14_1', 'x_14_2', 'x_14_3', 'x_14_4', 'x_14_5', 'x_21_B', 'x_21_C', 'x_21_D', 'x_21_E', 'x_21_F', 'x_21_G', 'x_21_H', 'x_21_I', 'x_21_J', 'x_21_K', 'x_21_L', 'x_21_M', 'x_21_N', 'x_21_O', 'x_21_P', 'x_24_B', 'x_24_C', 'x_24_D', 'x_24_E']\n",
      "ATE estimate: [-498.4131974]\n",
      "ATE robust (sandwich) standard error: [10.75614945]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"more_covariates_balanced.csv\", index_col=0)\n",
    "\n",
    "# \"A ~ x_1 + x_3 + x_4 + x_5 + x_6 + x_7 + x_8 + x_9 + x_10 + x_11 + x_12 + x_13 + x_14 + x_15 + x_16 + x_17 + x_18 + x_19 + x_20 \n",
    "# + x_21 + x_23 + x_25 + x_26 + x_27 + x_28 + x_29 + x_30 + x_31 + \n",
    "# x_32 + x_33 + x_34 + x_35 + x_36 + x_37 + x_38 + x_39 + x_40 + x_41 + x_42 + x_43 \n",
    "# + x_44 + x_45 + x_46 + x_47 + x_48 + x_49 + x_50 + x_51 + x_52 + x_53 + x_54 + x_55 + x_56 + x_57 + x_58\"\n",
    "\n",
    "\n",
    "categorical_cols = ['x_11', 'x_14', 'x_21', 'x_24']\n",
    "\n",
    "\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
    "print(data_encoded.columns.to_list())\n",
    "x_cols = ['x_1', 'x_3', 'x_4', 'x_5', 'x_6', 'x_7', 'x_8', 'x_9', 'x_10', 'x_12', 'x_13', 'x_15', 'x_16', 'x_17', 'x_18', \n",
    "          'x_19', 'x_20', 'x_23', 'x_25', 'x_26', 'x_27', 'x_28', 'x_29', 'x_30', 'x_31', 'x_32', 'x_33', 'x_34', 'x_35', 'x_36', \n",
    "          'x_37', 'x_38', 'x_39', 'x_40', 'x_41', 'x_42', 'x_43', 'x_44', 'x_45', 'x_46', 'x_47', 'x_48', 'x_49', 'x_50', 'x_51', 'x_52', 'x_53', 'x_54', 'x_55', 'x_56', 'x_57', 'x_58',\n",
    "          'x_11_1', 'x_11_2', 'x_11_3', 'x_14_1', 'x_14_2', 'x_14_3', 'x_14_4', 'x_14_5', 'x_21_B', 'x_21_C', 'x_21_D', 'x_21_E', 'x_21_F', 'x_21_G', 'x_21_H', 'x_21_I', 'x_21_J', 'x_21_K', 'x_21_L', 'x_21_M', 'x_21_N', 'x_21_O', 'x_21_P', 'x_24_B', 'x_24_C', 'x_24_D', 'x_24_E']\n",
    "dml_data = DoubleMLData(data_encoded, y_col='Y', d_cols='A', x_cols=x_cols)\n",
    "\n",
    "# ml_g = LogisticRegressionCV(cv=5)\n",
    "ml_g = LinearRegression(cv=5)\n",
    "ml_m = LassoCV(cv=5)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "dml_plr = DoubleMLPLR(dml_data, ml_g, ml_m)\n",
    "\n",
    "dml_plr.fit()\n",
    "\n",
    "\n",
    "ate_estimate = dml_plr.coef\n",
    "ate_se = dml_plr.se\n",
    "\n",
    "print(\"ATE estimate:\", ate_estimate)\n",
    "print(\"ATE robust (sandwich) standard error:\", ate_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE estimate: [-508.1181795]\n",
      "ATE robust (sandwich) standard error: [23.07124979]\n",
      "         coef   std err          t          P>|t|       2.5 %      97.5 %\n",
      "A -508.118179  23.07125 -22.023869  1.701034e-107 -553.336998 -462.899361\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "data = pd.read_csv(\"more_covariates_balanced.csv\", index_col=0)\n",
    "\n",
    "# \"A ~ x_1 + x_3 + x_4 + x_5 + x_6 + x_7 + x_8 + x_9 + x_10 + x_11 + x_12 + x_13 + x_14 + x_15 + x_16 + x_17 + x_18 + x_19 + x_20 \n",
    "# + x_21 + x_23 + x_25 + x_26 + x_27 + x_28 + x_29 + x_30 + x_31 + \n",
    "# x_32 + x_33 + x_34 + x_35 + x_36 + x_37 + x_38 + x_39 + x_40 + x_41 + x_42 + x_43 \n",
    "# + x_44 + x_45 + x_46 + x_47 + x_48 + x_49 + x_50 + x_51 + x_52 + x_53 + x_54 + x_55 + x_56 + x_57 + x_58\"\n",
    "\n",
    "\n",
    "categorical_cols = ['x_11', 'x_14', 'x_21', 'x_24']\n",
    "\n",
    "\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
    "# print(data_encoded.columns.to_list())\n",
    "x_cols = ['x_1', 'x_3', 'x_4', 'x_5', 'x_6', 'x_7', 'x_8', 'x_9', 'x_10', 'x_12', 'x_13', 'x_15', 'x_16', 'x_17', 'x_18', \n",
    "          'x_19', 'x_20', 'x_23', 'x_25', 'x_26', 'x_27', 'x_28', 'x_29', 'x_30', 'x_31', 'x_32', 'x_33', 'x_34', 'x_35', 'x_36', \n",
    "          'x_37', 'x_38', 'x_39', 'x_40', 'x_41', 'x_42', 'x_43', 'x_44', 'x_45', 'x_46', 'x_47', 'x_48', 'x_49', 'x_50', 'x_51', 'x_52', 'x_53', 'x_54', 'x_55', 'x_56', 'x_57', 'x_58',\n",
    "          'x_11_1', 'x_11_2', 'x_11_3', 'x_14_1', 'x_14_2', 'x_14_3', 'x_14_4', 'x_14_5', 'x_21_B', 'x_21_C', 'x_21_D', 'x_21_E', 'x_21_F', 'x_21_G', 'x_21_H', 'x_21_I', 'x_21_J', 'x_21_K', 'x_21_L', 'x_21_M', 'x_21_N', 'x_21_O', 'x_21_P', 'x_24_B', 'x_24_C', 'x_24_D', 'x_24_E']\n",
    "dml_data = DoubleMLData(data_encoded, y_col='Y', d_cols='A', x_cols=x_cols)\n",
    "\n",
    "ml_m = MLPRegressor(hidden_layer_sizes=(32, 32), activation='relu', solver='adam', max_iter=500, random_state=42, early_stopping=True, alpha=0.01)\n",
    "\n",
    "\n",
    "ml_g = MLPClassifier(hidden_layer_sizes=(32, 32), activation='relu', solver='adam', max_iter=500, random_state=42, early_stopping=True, alpha=0.01)\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "dml_plr = DoubleMLPLR(dml_data, ml_m, ml_g, n_folds=5)\n",
    "\n",
    "\n",
    "dml_plr.fit()\n",
    "\n",
    "\n",
    "ate_estimate = dml_plr.coef\n",
    "ate_se = dml_plr.se\n",
    "\n",
    "print(\"ATE estimate:\", ate_estimate)\n",
    "print(\"ATE robust (sandwich) standard error:\", ate_se)\n",
    "\n",
    "est2 = dml_plr.summary\n",
    "print(est2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         coef   std err           t  P>|t|       2.5 %      97.5 %\n",
      "A -499.287369  1.558794 -320.303726    0.0 -502.342548 -496.232189\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "aipw_obj_1 = DoubleMLIRM(dml_data, ml_g = XGBRegressor(),\n",
    "                      ml_m = XGBClassifier(), n_folds=5)\n",
    "aipw_obj_1.fit()\n",
    "\n",
    "aipw_est_1 = aipw_obj_1.summary\n",
    "print(aipw_est_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h-IPTW estimator: -447.30115788717376\n",
      "Standard deviation of the h-IPTW estimator: 99.33813056960963\n",
      "h-IPTW estimator - 3 * std:  -745.3155495960027\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "data = pd.read_csv(\"more_covariates_balanced.csv\", index_col=0)\n",
    "\n",
    "\n",
    "categorical_cols = ['x_11', 'x_14', 'x_21', 'x_24']\n",
    "\n",
    "\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
    "def compute_h_iptw(df):\n",
    "    predictors = ['x_1', 'x_3', 'x_4', 'x_5', 'x_6', 'x_7', 'x_8', 'x_9', 'x_10', 'x_12', 'x_13', 'x_15', 'x_16', 'x_17', 'x_18', \n",
    "          'x_19', 'x_20', 'x_23', 'x_25', 'x_26', 'x_27', 'x_28', 'x_29', 'x_30', 'x_31', 'x_32', 'x_33', 'x_34', 'x_35', 'x_36', \n",
    "          'x_37', 'x_38', 'x_39', 'x_40', 'x_41', 'x_42', 'x_43', 'x_44', 'x_45', 'x_46', 'x_47', 'x_48', 'x_49', 'x_50', 'x_51', 'x_52', 'x_53', 'x_54', 'x_55', 'x_56', 'x_57', 'x_58',\n",
    "          'x_11_1', 'x_11_2', 'x_11_3', 'x_14_1', 'x_14_2', 'x_14_3', 'x_14_4', 'x_14_5', 'x_21_B', 'x_21_C', 'x_21_D', 'x_21_E', 'x_21_F', 'x_21_G', 'x_21_H', 'x_21_I', 'x_21_J', 'x_21_K', 'x_21_L', 'x_21_M', 'x_21_N', 'x_21_O', 'x_21_P', 'x_24_B', 'x_24_C', 'x_24_D', 'x_24_E']\n",
    "    X_ps = df[predictors]\n",
    "    A = df['A']\n",
    "    Y = df['Y']\n",
    "    \n",
    "    propensity_model = LogisticRegression(solver='liblinear')\n",
    "    propensity_model.fit(X_ps, A)\n",
    "    propensity_scores = propensity_model.predict_proba(X_ps)[:, 1]\n",
    "    \n",
    "    eps = 1e-6\n",
    "    propensity_scores = np.clip(propensity_scores, eps, 1 - eps)\n",
    "    \n",
    "    treated_numerator = np.sum(Y * A / propensity_scores)\n",
    "    treated_denominator = np.sum(A / propensity_scores)\n",
    "    treated_mean = treated_numerator / treated_denominator\n",
    "\n",
    "    control_numerator = np.sum(Y * (1 - A) / (1 - propensity_scores))\n",
    "    control_denominator = np.sum((1 - A) / (1 - propensity_scores))\n",
    "    control_mean = control_numerator / control_denominator\n",
    "    \n",
    "    return treated_mean - control_mean\n",
    "\n",
    "tau_h_iptw = compute_h_iptw(data_encoded)\n",
    "print(\"h-IPTW estimator:\", tau_h_iptw)\n",
    "\n",
    "B = 1000\n",
    "bootstrap_estimates = []\n",
    "n = data.shape[0]\n",
    "\n",
    "for b in range(B):\n",
    "    bootstrap_sample = data_encoded.sample(n, replace=True)\n",
    "    estimate = compute_h_iptw(bootstrap_sample)\n",
    "    bootstrap_estimates.append(estimate)\n",
    "\n",
    "\n",
    "std_est = np.std(bootstrap_estimates, ddof=1)\n",
    "print(\"Standard deviation of the h-IPTW estimator:\", std_est)\n",
    "print(\"h-IPTW estimator - 3 * std: \", tau_h_iptw - 3 * std_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATE (beta): -577.7947922921327\n",
      "Coefficient on estimated propensity (gamma): -809.8494440031762\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, LassoLars\n",
    "\n",
    "np.random.seed(42)\n",
    "data = pd.read_csv(\"more_covariates_balanced.csv\", index_col=0)\n",
    "\n",
    "\n",
    "\n",
    "categorical_cols = ['x_11', 'x_14', 'x_21', 'x_24']\n",
    "\n",
    "\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
    "# print(data_encoded.columns.to_list())\n",
    "x_cols = ['x_1', 'x_3', 'x_4', 'x_5', 'x_6', 'x_7', 'x_8', 'x_9', 'x_10', 'x_12', 'x_13', 'x_15', 'x_16', 'x_17', 'x_18', \n",
    "          'x_19', 'x_20', 'x_23', 'x_25', 'x_26', 'x_27', 'x_28', 'x_29', 'x_30', 'x_31', 'x_32', 'x_33', 'x_34', 'x_35', 'x_36', \n",
    "          'x_37', 'x_38', 'x_39', 'x_40', 'x_41', 'x_42', 'x_43', 'x_44', 'x_45', 'x_46', 'x_47', 'x_48', 'x_49', 'x_50', 'x_51', 'x_52', 'x_53', 'x_54', 'x_55', 'x_56', 'x_57', 'x_58',\n",
    "          'x_11_1', 'x_11_2', 'x_11_3', 'x_14_1', 'x_14_2', 'x_14_3', 'x_14_4', 'x_14_5', 'x_21_B', 'x_21_C', 'x_21_D', 'x_21_E', 'x_21_F', 'x_21_G', 'x_21_H', 'x_21_I', 'x_21_J', 'x_21_K', 'x_21_L', 'x_21_M', 'x_21_N', 'x_21_O', 'x_21_P', 'x_24_B', 'x_24_C', 'x_24_D', 'x_24_E']\n",
    "# dml_data = DoubleMLData(data_encoded, y_col='Y', d_cols='A', x_cols=x_cols)\n",
    "\n",
    "\n",
    "\n",
    "ml_g = LogisticRegression(solver='liblinear', random_state=42)\n",
    "# mlg = MLPClassifier(hidden_layer_sizes=(64, 64), activation='relu', solver='adam', max_iter=500, random_state=42, early_stopping=True, alpha=0.01)\n",
    "ml_g.fit(data_encoded[x_cols], data_encoded['A'])\n",
    "pi_hat = ml_g.predict_proba(data_encoded[x_cols])[:, 1]\n",
    "\n",
    "\n",
    "data_encoded['pi_hat'] = pi_hat\n",
    "\n",
    "\n",
    "X_reg = data_encoded[['A', 'pi_hat']]\n",
    "Y = data_encoded['Y']\n",
    "\n",
    "outcome_model = LinearRegression()\n",
    "outcome_model.fit(X_reg, Y)\n",
    "beta_hat = outcome_model.coef_[0]\n",
    "gamma_hat = outcome_model.coef_[1]\n",
    "\n",
    "print(\"Estimated ATE (beta):\", beta_hat)\n",
    "print(\"Coefficient on estimated propensity (gamma):\", gamma_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATE: -478.4445462026051\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"new_datAY_2.csv\", index_col=0)\n",
    "categorical_cols = ['x_11', 'x_14', 'x_21', 'x_24']\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "treated = data_encoded[data_encoded['A'] == 1]\n",
    "control = data_encoded[data_encoded['A'] == 0]\n",
    "\n",
    "model_treated = LinearRegression()\n",
    "model_treated.fit(treated[x_cols], treated['Y'])\n",
    "\n",
    "model_control = LinearRegression()\n",
    "model_control.fit(control[x_cols], control['Y'])\n",
    "\n",
    "Y1_pred = model_treated.predict(data_encoded[x_cols])\n",
    "Y0_pred = model_control.predict(data_encoded[x_cols])\n",
    "\n",
    "ate_estimate = np.mean(Y1_pred - Y0_pred)\n",
    "print(\"Estimated ATE:\", ate_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATE: -472.7975019844525\n"
     ]
    }
   ],
   "source": [
    "treated = data_encoded[data_encoded['A'] == 1]\n",
    "control = data_encoded[data_encoded['A'] == 0]\n",
    "\n",
    "\n",
    "model_treated = LassoCV()\n",
    "model_treated.fit(treated[x_cols], treated['Y'])\n",
    "\n",
    "\n",
    "model_control = LassoCV(cv=5)\n",
    "model_control.fit(control[x_cols], control['Y'])\n",
    "\n",
    "\n",
    "Y1_pred = model_treated.predict(data_encoded[x_cols])\n",
    "Y0_pred = model_control.predict(data_encoded[x_cols])\n",
    "\n",
    "ate_estimate = np.mean(Y1_pred - Y0_pred)\n",
    "print(\"Estimated ATE:\", ate_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATE: -388.4398355342925\n"
     ]
    }
   ],
   "source": [
    "X = data_encoded[['A'] + x_cols]\n",
    "y = data_encoded['Y']\n",
    "\n",
    "model = LassoCV(cv=5)\n",
    "model.fit(X, y)\n",
    "\n",
    "X_treated = X.copy()\n",
    "X_control = X.copy()\n",
    "X_treated['A'] = 1\n",
    "X_control['A'] = 0\n",
    "\n",
    "\n",
    "Y1_pred = model.predict(X_treated)\n",
    "Y0_pred = model.predict(X_control)\n",
    "\n",
    "ate_estimate = np.mean(Y1_pred - Y0_pred)\n",
    "print(\"Estimated ATE:\", ate_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/choudhury/miniconda3/envs/post_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATE: -603.4135714812794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/choudhury/miniconda3/envs/post_env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "treated = data_encoded[data_encoded['A'] == 1]\n",
    "control = data_encoded[data_encoded['A'] == 0]\n",
    "\n",
    "\n",
    "model_treated = MLPRegressor(hidden_layer_sizes=(32, 32), activation='relu', solver='adam', max_iter=500, random_state=42, early_stopping=True, alpha=0.01)\n",
    "model_treated.fit(treated[x_cols], treated['Y'])\n",
    "\n",
    "\n",
    "model_control = MLPRegressor(hidden_layer_sizes=(32, 32), activation='relu', solver='adam', max_iter=500, random_state=42, early_stopping=True, alpha=0.01)\n",
    "model_control.fit(control[x_cols], control['Y'])\n",
    "\n",
    "\n",
    "Y1_pred = model_treated.predict(data_encoded[x_cols])\n",
    "Y0_pred = model_control.predict(data_encoded[x_cols])\n",
    "\n",
    "\n",
    "ate_estimate = np.mean(Y1_pred - Y0_pred)\n",
    "print(\"Estimated ATE:\", ate_estimate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "post_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
